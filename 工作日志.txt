5.29
距离实验截止还有3天

FC的问题在于误码率和code的选择难以控制。现在的FRR不稳定，忽高忽低的原因有
1. gallery data的表现能力可能不足
2. biohashing产生的template可能存在大量误码
3. fuzzy commitment纠错能力有限

period 1..
1. 优化代码，将用户数据记录为table并按行输入 ：）
2. 明确是否需要更换hashing还是ecc，测量template ber的均值 ：）

period 2..
3. bioinfo的图表也可以出
3. （performance evaluation)横向比较hashing的ber，比较legitimate和intruder误码的histogram
4. 研究是否需要新的hashing

5.30
使用了更conservative的BCH后，frr有了显著下降。
误码率基本上保持稳定分布，似乎与gallery pool大小有关，与frequency mask有关。

1. 将framework写成函数是有用的，然而，应该不是当务之急
2. 加上RS code和IoM hashing同理，有用但不是当务之急
3. 当务之急是可以展示的结果
	biometric information: insightful

注意：BER的分析可以根据同一用户和不同用户的误码差异，决定ECC能力

	(ok)	BER展示了biohashing与intra-subject variation，属于accuracy
	(ok)	FRR展示了ECC纠正BER的能力，属于accuracy
	(ok)	template collision是防止传输过程中的injection attack的，属于security
	(no ok) 冒名攻击，(假冒输入数据）使用legitiate user的helper data，属于security
！！ 出问题了 ！！ 
spoofing attack没法防御，可能是bmask的原因，也有可能是biohashing中含0元素过多的问题,也可能是gallery data数量的问题。

attempt: 1. 增加gallery data，仍然是居中于0.5的分布
2. 增加码长，frr降低了，但far似乎没有变化

这说明它不是因为每一位0.5的分布而造成的，增加码长仍然没有增加这个推断难度。说明独特的信息被shortcut了。

4. 系统重新搭建运行，划分成几个逻辑块，编写成脚本
5. github上线

	(no ok) 假人攻击，security

	噪声环境：内噪，外噪（也是在假人上做） => robustness

	efficiency: computation/communication/storage overhead
	sparse IoM!! 值得一试，进一步减少overhead

	system building credit

4. ablation study and system parameter tuning

6.3
我也不知道为什么突然荒废了3天。
系统设计的问题似乎都不存在了，因为SAR本来就取决于数据本身的相似程度。

bch码纠错能力的问题通过bchnumerr(n)来查询所需要的纠错能力，需要从enroll feaeture中draw几个样本来得到threshold.
其中需要平衡Frr和sar(我不清楚从pdf中draw会比较复杂，文章里可以写）

教训：版本管理，必须上github，不然每一次都有可能丢失code
由intrution造成的误码率与数据本身的相似度有关
由random guess造成的误码率是pq=0.25
用自己的template造成的误码率与hashing有关

I don't think RS code is a contribution.